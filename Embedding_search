import vertexai
from vertexai.preview.language_models import TextEmbeddingModel
from google.cloud import bigquery
from scipy.spatial.distance import cosine
from datetime import timedelta
from dataclasses import dataclass, field
from typing import List
from vertexai.preview.generative_models import GenerativeModel, GenerationConfig, SafetySetting, HarmCategory, HarmBlockThreshold
from vertexai.preview import caching

# Initialize Vertex AI
vertexai.init(project="prj-cdw-p-aiml-001-0b36", location="us-central1")  # Replace with your project ID and location

# BigQuery client
client = bigquery.Client()

# BigQuery table path
table_path = 'prj-cdw-p-aiml-001-0b36.Sonny_Workspace.MCG_Chunks_ALL'

# Vertex AI model IDs
embedding_model_id = "textembedding-gecko@001"  # For embeddings

@dataclass
class GeminiConfig:
    """Gemini model configurations."""
    project_id: str
    temperature: float = 0.9
    top_p: float = 0.8
    top_k: int = 40
    max_output_tokens: int = 2048
    candidate_count: int = 1
    location: str = "us-central1"
    model_name: str = "gemini-1.5-flash-001"
    system_instruction: List = field(default_factory=list)
    cc_name: str = None
    safety_config: list = None

def gemini_pro_generate_text(content: str, gemini_config: GeminiConfig):
    """Generate text using Gemini models."""
    vertexai.init(project=gemini_config.project_id, location=gemini_config.location)
    generation_config = GenerationConfig(
        temperature=gemini_config.temperature,
        top_p=gemini_config.top_p,
        top_k=gemini_config.top_k,
        candidate_count=gemini_config.candidate_count,
        max_output_tokens=gemini_config.max_output_tokens,
        response_mime_type="application/json",
    )
    model = GenerativeModel(gemini_config.model_name, system_instruction=gemini_config.system_instruction)
    response = model.generate_content(content, generation_config=generation_config, stream=False)
    return response.candidates[0].content.parts[0].text

def get_embeddings_from_bigquery(table_path, model_id):
    """Fetch embeddings for chunks in a BigQuery table."""
    query = f"""
        SELECT chunk_id, chunk_text
        FROM `{table_path}`
    """
    query_job = client.query(query)
    results = query_job.result()

    # Load Vertex AI embedding model
    model = TextEmbeddingModel.from_pretrained(model_id)

    # Create embeddings
    embeddings = {}
    chunk_texts = {}
    for row in results:
        embedding = model.get_embeddings([row.chunk_text])[0].values
        embeddings[row.chunk_id] = embedding
        chunk_texts[row.chunk_id] = row.chunk_text

    return embeddings, chunk_texts

def get_similar_chunks(query_text, embeddings, chunk_texts, top_k=10):
    """Find the most similar chunks based on query text."""
    query_embedding = TextEmbeddingModel.from_pretrained(embedding_model_id).get_embeddings([query_text])[0].values

    # Calculate cosine similarity
    similarities = {}
    for chunk_id, embedding in embeddings.items():
        similarity = 1 - cosine(query_embedding, embedding)
        similarities[chunk_id] = similarity

    # Get top-k chunks
    sorted_chunks = sorted(similarities.items(), key=lambda item: item[1], reverse=True)[:top_k]
    top_chunks = {chunk_id: chunk_texts[chunk_id] for chunk_id, _ in sorted_chunks}

    return top_chunks

def generate_response(query_text, top_chunks):
    """Generate a response using RAG and Gemini."""
    # Construct prompt
    if not top_chunks:
        return "I couldn't find relevant context to answer your question."

    context = "\n\n".join([f"Chunk {chunk_id}: {chunk_text}" for chunk_id, chunk_text in top_chunks.items()])
    prompt = f"Answer the following question based on the provided context:\n\nQuestion: {query_text}\n\nContext:\n{context}"

    # Initialize GeminiConfig
    gemini_config = GeminiConfig(
        project_id="prj-cdw-p-aiml-001-0b36",
        model_name="gemini-1.5-flash-001",
        temperature=0.0,
        max_output_tokens=8000,
        top_p=0.1,
        top_k=10,
        location="us-central1",
    )

    # Generate response using Gemini
    try:
        response = gemini_pro_generate_text(
            content=prompt,
            gemini_config=gemini_config,
        )
        return response
    except Exception as e:
        return f"Error generating response: {str(e)}"

# Example user query
user_query = "What are symptoms of heart disease?"

# RAG workflow
embeddings, chunk_texts = get_embeddings_from_bigquery(table_path, embedding_model_id)
top_chunks = get_similar_chunks(user_query, embeddings, chunk_texts)
response = generate_response(user_query, top_chunks)

# Output the response
print(response)
